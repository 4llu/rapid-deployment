#? Creation
name: time
log: true
save: false # TODO
eval_freq: 1 # * Decrease for hyper opt and final testing
eval_episodes: 4 # * Increase for hyper opt and final testing
# eval_episodes: 15 # * Increase for hyper opt and final testing

#? Fewshot setup

k_shot: 1
n_way: 10
n_query: 30
mix_rpms: true
mix_sensors: false
separate_query_and_support: false

#? Data

data: ARotor
# train_sensors: [torque1, acc1, acc2, acc3, acc4, en1acc_grad, en2acc_grad, en3acc_grad, en4acc_grad, en5acc_grad]
# train_sensors: [torque1]
# validation_sensors: [torque1]
# test_sensors: [torque2]
train_sensors: [en1speed_grad, en2speed_grad, en3speed_grad, en5speed_grad]
validation_sensors: [en4speed_grad]
test_sensors: [en3speed_grad]
train_rpm: [250, 500, 750]
validation_rpm: [500, 750]
test_rpm: [1250, 1500]
# train_rpm: [250, 1500]
# validation_rpm: [750, 1250]
# test_rpm: [500, 1250]
# train_rpm: [250, 750, 1500]
# validation_rpm: [500, 1000]
# test_rpm: [500, 1250]
# train_rpm: [500, 1000, 1500]
# validation_rpm: [250]
# test_rpm: [750, 1250]
# train_rpm: [500, 1250]
# validation_rpm: [250, 1500]
# test_rpm: [750, 1000]
train_classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
validation_classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
test_classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
# window_width: 12984
# window_width: 8656
# window_width: 17312
window_width: 19476
# window_width: 4328
window_overlap: 0.99

#? Preprocessing

# preprocessing_full: []
# preprocessing_full: [robust_scaling]
# preprocessing_full: [lowpass_filtering]
preprocessing_full: [lowpass_filtering, robust_scaling]
lp_filter_cutoff: 200 #! lowpass doesn't use this currently!
robust_scaling_low: 0.25
robust_scaling_high: 0.75
# mixed_query_normalization_scale: 0.1
preprocessing_sample: []
# preprocessing_sample: [FFT]
# preprocessing_sample: [sync_FFT] # ! Slow, always prefer the other two if possible
# sync_FFT_rotations: 8 # How many rotations to include for synced FFT. Only used if sync_FFT on.
preprocessing_class_batch: [TSA]
TSA_rotations: 3
TSA_cycles: 3
# preprocessing_class_batch: [FFT]
# preprocessing_class_batch: [FFT, FFT_mean_std_channels]
# preprocessing_batch: []
preprocessing_batch: [hilbert_envelope]
# preprocessing_batch: [FFT]
# preprocessing_batch: [gain_changer, add_white_noise]
# preprocessing_batch: [FFT, gain_changer, block_shuffle]
# preprocessing_batch: [FFT, gain_changer, mult_white_noise, block_shuffle]
# preprocessing_batch: [FFT, gain_changer, mult_white_noise, block_shuffle, combined_freq_masking]
# preprocessing_batch: [FFT, gain_changer]
gain_std: 0.2
white_noise_std: 0.02
# combined_freq_masking_probabilities: [0.7, 0.3, 0, 0]
# block_size: 2048
# preprocessing_batch: [sync_FFT]
include_FFT_DC: false # Should the DC component of the FFC be included? Only relevant if FFT is used in some form
log_FFT: false
pad_FFT: -1
sample_cut: -1 # Done in batch preprocessing
# pad_FFT: 8192

# FFT: true
# embedding_unit_len: true
# rpm_status: batch
# sensor_status: batch
# normalize: false

#? Model

model: prototypical
backbone: InceptionTime
# backbone: WDCNN
# distance_network: simple
# distance_network: default
kaiming_init: true
embedding_len: 64
# embedding_normalization_type: False
# Legacy
# embedding_normalization_type: old
# Lp-norm
embedding_normalization_type: lp
lp_norm: 2
# Mahalanobis
# embedding_normalization_type: mahalanobis # TODO

embedding_multiplier: 100

#? Training

epoch_len: 1
max_epochs: 200
patience: 1000
optimizer: AdamW
# lr: 0.003
# lr: 0.0001
lr: 0.00442
lr_scheduler: ExponentialLR
# sch_gamma: 0.9999999
# sch_gamma: 0.9998
# sch_gamma: 0.99
sch_gamma: 0.9641
momentum: 0.9859
# Regularization
weight_decay: 0.0001729
# weight_decay: 0.0001
cl_dropout: 0.0
fc_dropout: 0.0

# ? Other

use_amp: false  